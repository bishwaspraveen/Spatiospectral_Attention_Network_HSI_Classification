{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Necessary Imports\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "import scipy.io as sio\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "import os\n",
    "import random\n",
    "from random import shuffle\n",
    "import scipy.ndimage\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.activations import softmax\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Conv3D, Reshape, BatchNormalization, MaxPooling3D, Bidirectional, LSTM\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Nadam, Adadelta, Adagrad, Adamax\n",
    "from tensorflow.keras import backend as K\n",
    "#K.set_image_dim_ordering('th')\n",
    "import np_utils\n",
    "from tensorflow.keras.utils import plot_model\n",
    "#from keract import get_activations, display_activations\n",
    "from tensorflow import one_hot as to_categorical\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, cohen_kappa_score, accuracy_score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.io as sci\n",
    "import time\n",
    "from sklearn import random_projection as rp\n",
    "#from skimage.filters import gabor, gaussian\n",
    "import scipy\n",
    "import spectral\n",
    "import time\n",
    "import math\n",
    "import pydot\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Specify Window size, Train-Test-Validation split\n",
    "\n",
    "windowSize = 11\n",
    "testRatio = 0.9\n",
    "valRatio = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to load Dataset\n",
    "\n",
    "def loadIndianPinesData():\n",
    "    data = sio.loadmat(r'C:/Users/bishw/OneDrive/Desktop/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "    labels = sio.loadmat(r'C:/Users/bishw/OneDrive/Desktop/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = loadIndianPinesData()\n",
    "print (\"Shape of X : \"+str(X.shape))\n",
    "print (\"Shape of y : \"+str(y.shape))\n",
    "\n",
    "#For plotting accuracy plots\n",
    "y_full = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to normalise dataset\n",
    "\n",
    "def standartizeData(X):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    scaler = preprocessing.StandardScaler()  \n",
    "    newX = scaler.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1],X.shape[2]))\n",
    "    return newX, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, scaler = standartizeData(X)\n",
    "print (\"Shape of X : \"+str(X.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to apply PCA onto the dataset for dimensionality reduction\n",
    "\n",
    "def applyPCA(X, numComponents=50):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, pca = applyPCA(X)\n",
    "\n",
    "print (\"Shape of X : \"+str(X.shape))\n",
    "\n",
    "#For plotting accuracy plots\n",
    "X_full = X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to pad input windows with 0's if necessary for convolution\n",
    "\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "#Function to create windows for input data points\n",
    "\n",
    "def createPatches(X, y, windowSize=7, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))\n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))\n",
    "    patchIndex = 0\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]   \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]\n",
    "            patchIndex = patchIndex + 1\n",
    "    if removeZeroLabels:\n",
    "        patchesData = patchesData[patchesLabels>0,:,:,:]\n",
    "        patchesLabels = patchesLabels[patchesLabels>0]\n",
    "        patchesLabels -= 1\n",
    "    return patchesData, patchesLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = createPatches(X, y, windowSize=windowSize)\n",
    "print (\"Shape of X : \"+str(X.shape))\n",
    "print (\"Shape of y : \"+str(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to split the dataset into training, testing and validation samples\n",
    "\n",
    "def splitTrainTestSet(X, y, testRatio=0.9):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=testRatio, random_state=345, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=testRatio/(testRatio + valRatio), random_state=345, stratify=y_test)\n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test,X_val, y_train, y_test, y_val = splitTrainTestSet(X, y, testRatio)\n",
    "print (\"Shape of X_train : \"+str(X_train.shape))\n",
    "print (\"Shape of X_test : \"+str(X_test.shape))\n",
    "print (\"Shape of X_val : \"+str(X_val.shape))\n",
    "print (\"Shape of y_train : \"+str(y_train.shape))\n",
    "print (\"Shape of y_test : \"+str(y_test.shape))\n",
    "print (\"Shape of y_val : \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if the dataset has a class imbalance problem\n",
    "\n",
    "count = y_train\n",
    "import collections\n",
    "counter=collections.Counter(count)\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function definition to oversample weaker classes in the dataset\n",
    "\n",
    "def oversampleWeakClasses(X, y):\n",
    "    uniqueLabels, labelCounts = np.unique(y, return_counts=True)\n",
    "    maxCount = np.max(labelCounts)\n",
    "    labelInverseRatios = maxCount / labelCounts  \n",
    "    # repeat for every label and concat\n",
    "    newX = X[y == uniqueLabels[0], :, :, :].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    newY = y[y == uniqueLabels[0]].repeat(round(labelInverseRatios[0]), axis=0)\n",
    "    for label, labelInverseRatio in zip(uniqueLabels[1:], labelInverseRatios[1:]):\n",
    "        cX = X[y== label,:,:,:].repeat(round(labelInverseRatio), axis=0)\n",
    "        cY = y[y == label].repeat(round(labelInverseRatio), axis=0)\n",
    "        newX = np.concatenate((newX, cX))\n",
    "        newY = np.concatenate((newY, cY))\n",
    "    np.random.seed(seed=42)\n",
    "    rand_perm = np.random.permutation(newY.shape[0])\n",
    "    newX = newX[rand_perm, :, :, :]\n",
    "    newY = newY[rand_perm]\n",
    "    return newX, newY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,  y_train = oversampleWeakClasses(X_train, y_train)\n",
    "print (\"Shape of X_train : \"+str(X_train.shape))\n",
    "print (\"Shape of X_test : \"+str(X_test.shape))\n",
    "print (\"Shape of X_val : \"+str(X_val.shape))\n",
    "print (\"Shape of y_train : \"+str(y_train.shape))\n",
    "print (\"Shape of y_test : \"+str(y_test.shape))\n",
    "print (\"Shape of y_val : \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is now visible that the all the classes in the dataset have about the same number of data points\n",
    "\n",
    "count = y_train\n",
    "import collections\n",
    "counter=collections.Counter(count)\n",
    "print (counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function defintion to augment data samples in the dataset\n",
    "\n",
    "def AugmentData(X_train):\n",
    "    for i in range(int(X_train.shape[0]/2)):\n",
    "        patch = X_train[i,:,:,:]\n",
    "        num = random.randint(0,2)\n",
    "        if (num == 0):\n",
    "            \n",
    "            flipped_patch = np.flipud(patch)\n",
    "        if (num == 1):\n",
    "            \n",
    "            flipped_patch = np.fliplr(patch)\n",
    "        if (num == 2):\n",
    "            \n",
    "            no = random.randrange(-180,180,30)\n",
    "            flipped_patch = scipy.ndimage.interpolation.rotate(patch, no,axes=(1, 0),\n",
    "                                                               reshape=False, output=None, order=3, mode='constant', cval=0.0, prefilter=False)\n",
    "    \n",
    "    \n",
    "    patch2 = flipped_patch\n",
    "    X_train[i,:,:,:] = patch2\n",
    "    \n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = AugmentData(X_train)\n",
    "print (\"Shape of X_train : \"+str(X_train.shape))\n",
    "print (\"Shape of X_test : \"+str(X_test.shape))\n",
    "print (\"Shape of X_val : \"+str(X_val.shape))\n",
    "print (\"Shape of y_train : \"+str(y_train.shape))\n",
    "print (\"Shape of y_test : \"+str(y_test.shape))\n",
    "print (\"Shape of y_val : \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_RNN = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], X_train.shape[3], 1)\n",
    "X_test_RNN = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], X_test.shape[3], 1)\n",
    "X_val_RNN = X_val.reshape(X_val.shape[0], X_val.shape[1], X_val.shape[2], X_val.shape[3], 1)\n",
    "\n",
    "print (\"Shape of X_train_RNN : \"+str(X_train_RNN.shape))\n",
    "print (\"Shape of X_test_RNN : \"+str(X_test_RNN.shape))\n",
    "print (\"Shape of X_val_RNN : \"+str(X_val_RNN.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_RES = X_train\n",
    "X_test_RES = X_test\n",
    "X_val_RES = X_val\n",
    "\n",
    "print (\"Shape of X_train_RES : \"+str(X_train_RES.shape))\n",
    "print (\"Shape of X_test_RES : \"+str(X_test_RES.shape))\n",
    "print (\"Shape of X_val_RES : \"+str(X_val_RES.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the truth labels to categorical vectors for the training process\n",
    "\n",
    "y_train = tf.cast(y_train, tf.int32)\n",
    "y_test = tf.cast(y_test, tf.int32)\n",
    "y_val = tf.cast(y_val, tf.int32)\n",
    "\n",
    "y_train = to_categorical(indices = y_train, depth = 16)\n",
    "y_test = to_categorical(indices = y_test, depth = 16)\n",
    "y_val = to_categorical(indices = y_val, depth = 16)\n",
    "\n",
    "y_train = y_train.numpy()\n",
    "y_test = y_test.numpy()\n",
    "y_val = y_val.numpy()\n",
    "\n",
    "print (\"Shape of X_train_RNN : \"+str(X_train_RNN.shape))\n",
    "print (\"Shape of X_test_RNN : \"+str(X_test_RNN.shape))\n",
    "print (\"Shape of X_val_RNN : \"+str(X_val_RNN.shape))\n",
    "\n",
    "print (\"Shape of X_train_RES : \"+str(X_train_RES.shape))\n",
    "print (\"Shape of X_test_RES : \"+str(X_test_RES.shape))\n",
    "print (\"Shape of X_val_RES : \"+str(X_val_RES.shape))\n",
    "\n",
    "print (\"Shape of y_train : \"+str(y_train.shape))\n",
    "print (\"Shape of y_test : \"+str(y_test.shape))\n",
    "print (\"Shape of y_val : \"+str(y_val.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_RNN_shape = X_train_RNN[0].shape\n",
    "input_RES_shape = X_train_RES[0].shape\n",
    "\n",
    "inputRNN = Input(shape= input_RNN_shape)\n",
    "x = Conv3D(filters=16, kernel_size=(3,3,32), data_format='channels_last')(inputRNN)\n",
    "x = AveragePooling3D(pool_size=(2,2,2), data_format='channels_last')(x)\n",
    "x = Conv3D(filters=32, kernel_size=(3,3,8), data_format='channels_last')(x)\n",
    "x = Flatten()(x)\n",
    "x = Reshape((x.shape[1],1))(x)\n",
    "y = x\n",
    "x = Bidirectional(LSTM(1, return_sequences = True), merge_mode='mul')(x)\n",
    "x = Bidirectional(LSTM(1, return_sequences = True), merge_mode='mul')(x)\n",
    "x = Flatten()(x)\n",
    "x = Activation(activation='softmax')(x)\n",
    "x = Reshape((x.shape[1],1))(x)\n",
    "x = Multiply()([y, x])\n",
    "x = Add()([y, x])\n",
    "x = Flatten()(x)\n",
    "x = Dense(units = 100, activation='relu', kernel_regularizer='l2')(x)\n",
    "x = Dropout(rate = 0.2)(x)\n",
    "x = Dense(units = 50, activation='relu', kernel_regularizer='l2')(x)\n",
    "#x = Dense(units= 16, activation='softmax')(x)\n",
    "modelRNN = Model(inputs=inputRNN, outputs=x)\n",
    "\n",
    "#######################################################################################################################\n",
    "\n",
    "inputRES = Input(shape= input_RES_shape)\n",
    "y = inputRES\n",
    "y = Reshape((windowSize,windowSize,50,1))(y)\n",
    "y = Conv3D(filters=1, kernel_size=(1,1,25),activation='relu', data_format='channels_last')(y)\n",
    "y = Conv3D(filters=1, kernel_size=(1,1,26),activation='relu', data_format='channels_last')(y)\n",
    "y = Flatten()(y)\n",
    "y = Activation(activation='softmax')(y)\n",
    "y = Reshape((windowSize,windowSize,1))(y)\n",
    "#y = tf.keras.backend.repeat_elements(y, X_test_RES.shape[3], axis = 3)\n",
    "#y = repeat(y, repeats=X_test_RES.shape[3], axis = -1 )\n",
    "y = tf.tile(input=y, multiples=[1,1,1,X_test_RES.shape[3]])\n",
    "y = multiply([y, inputRES])\n",
    "y = Add()([y, inputRES])\n",
    "\n",
    "#y = Flatten()(y)\n",
    "#y = Dense(units=16, activation='softmax')(y)\n",
    "\n",
    "#First layer of ResNet\n",
    "skip1 = y\n",
    "skip1 = Conv2D(filters = 32, kernel_size=(3,3), strides=(1,1), padding='same', data_format='channels_last')(skip1)\n",
    "skip1 = BatchNormalization()(skip1)\n",
    "skip1 = Activation(activation='relu')(skip1)\n",
    "skip1 = Conv2D(filters = 64, kernel_size=(3,3), strides=(1,1), padding='same', data_format='channels_last')(skip1)\n",
    "skip1 = BatchNormalization()(skip1)\n",
    "y = Conv2D(filters = 64, kernel_size=(1,1), strides=(1,1), padding='same', data_format='channels_last')(y)\n",
    "y = Add()([y, skip1])\n",
    "y = Activation(activation='relu')(y)\n",
    "y = AveragePooling2D(pool_size=(2,2), data_format='channels_last')(y)\n",
    "\n",
    "#Second layer of ResNet\n",
    "skip2 = y\n",
    "skip2 = Conv2D(filters = 32, kernel_size=(3,3), strides=(1,1), padding='same', data_format='channels_last')(skip2)\n",
    "skip2 = BatchNormalization()(skip2)\n",
    "skip2 = Activation(activation='relu')(skip2)\n",
    "skip2 = Conv2D(filters = 64, kernel_size=(3,3), strides=(1,1), padding='same', data_format='channels_last')(skip2)\n",
    "skip2 = BatchNormalization()(skip2)\n",
    "y = Conv2D(filters = 64, kernel_size=(1,1), strides=(1,1), padding='same', data_format='channels_last')(y)\n",
    "y = Add()([y, skip2])\n",
    "y = Activation(activation='relu')(y)\n",
    "#extra addition\n",
    "#y = AveragePooling2D(pool_size=(2,2), data_format='channels_last')(y)\n",
    "\n",
    "#Third layer of ResNet\n",
    "skip3 = y\n",
    "skip3 = Conv2D(filters = 32, kernel_size=(3,3), strides=(1,1), padding='same', data_format='channels_last')(skip3)\n",
    "skip3 = BatchNormalization()(skip3)\n",
    "skip3 = Activation(activation='relu')(skip3)\n",
    "skip3 = Conv2D(filters = 64, kernel_size=(3,3), strides=(1,1), padding='same', data_format='channels_last')(skip3)\n",
    "skip3 = BatchNormalization()(skip3)\n",
    "y = Conv2D(filters = 64, kernel_size=(1,1), strides=(1,1), padding='same', data_format='channels_last')(y)\n",
    "y = Add()([y, skip3])\n",
    "y = Activation(activation='relu')(y)\n",
    "y = AveragePooling2D(pool_size=(2,2), data_format='channels_last')(y)\n",
    "\n",
    "#Dense Layers\n",
    "y = Flatten()(y)\n",
    "y = Dense(units = 100, activation='relu', kernel_regularizer='l2')(y)\n",
    "y = Dropout(rate = 0.2)(y)\n",
    "y = Dense(units= 50, activation='relu', kernel_regularizer='l2')(y)\n",
    "#y = Dense(units= 16, activation='softmax')(y)\n",
    "\n",
    "modelRES = Model(inputs=inputRES, outputs=y)\n",
    "\n",
    "#############################################################################################################################\n",
    "\n",
    "#Bringing the output of both the models together to final dense layers\n",
    "z = add([modelRNN.output, modelRES.output])\n",
    "z = Dense(units = 25, activation='relu', kernel_regularizer='l2')(z)\n",
    "z = Dropout(rate = 0.2)(z)\n",
    "z = Dense(units= y_train.shape[1], activation='softmax')(z)\n",
    "\n",
    "\n",
    "model = Model(inputs=[modelRNN.input, modelRES.input], outputs=z)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001, decay=1e-06)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x=[X_train_RNN, X_train_RES], y=y_train, epochs=100, batch_size=32, validation_data=([X_val_RNN, X_val_RES], y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the model's generalisation capability by testing it with the test-dataset samples\n",
    "\n",
    "score = model.evaluate(x = [X_test_RNN, X_test_RES], y=y_test, batch_size=32)\n",
    "Test_Loss =  score[0]\n",
    "Test_accuracy = score[1]*100\n",
    "print (\"Test Accuracy : \"+str(Test_accuracy))\n",
    "print (\"Test Loss : \"+str(Test_Loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "total = (end-start)/60\n",
    "print (\"Total time : \"+str(total)+\" minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training data', 'validation data'], loc='center right')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training data', 'validation data'], loc='center right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function definition to plot the accuracy maps\n",
    "\n",
    "def plot_accuracy_map (X, y):\n",
    "    Xshape = X\n",
    "    X, y = createPatches(X, y, windowSize=windowSize, removeZeroLabels=False)\n",
    "    X_RNN = X.reshape(X.shape[0], X.shape[1], X.shape[2], X.shape[3], 1)\n",
    "    X_RES = X\n",
    "    prediction = model.predict([X_RNN, X_RES])\n",
    "    prediction = np.argmax(prediction, axis = 1)\n",
    "    \n",
    "    for i in range(0, len(y)):\n",
    "        if (y[i] == 0):\n",
    "            y[i] = 0\n",
    "        elif (y[i] == 16):\n",
    "            y[i] = 17\n",
    "        else :\n",
    "            y[i] = prediction[i]+1\n",
    "            \n",
    "    y = y.reshape((Xshape.shape[0], Xshape.shape[1]))\n",
    "    predict_image = spectral.imshow(classes = y.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accuracy_map(X_full, y_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_image = spectral.imshow(classes = y_full.astype(int),figsize =(5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function definition to save reports onto a file\n",
    "\n",
    "def reports (X_test_RNN, X_test_RES, y_test):\n",
    "    prediction = model.predict([X_test_RNN, X_test_RES])\n",
    "    y_pred = np.argmax(prediction, axis=1)\n",
    "    target_names = ['Alfalfa', 'Corn-notill', 'Corn-mintill', 'Corn'\n",
    "               ,'Grass-pasture', 'Grass-trees', 'Grass-pasture-mowed', \n",
    "                'Hay-windrowed', 'Oats', 'Soybean-notill', 'Soybean-mintill',\n",
    "               'Soybean-clean', 'Wheat', 'Woods', 'Buildings-Grass-Trees-Drives',\n",
    "               'Stone-Steel-Towers']\n",
    "\n",
    "    \n",
    "    classification = classification_report(np.argmax(y_test, axis=1), y_pred, target_names=target_names, digits=3)\n",
    "    #confusion = confusion_matrix(np.argmax(y_test, axis=1), y_pred)\n",
    "    oa = accuracy_score(np.argmax(y_test, axis=1), y_pred)\n",
    "    score = model.evaluate([X_test_RNN, X_test_RES], y_test, batch_size=32)\n",
    "    Test_Loss =  score[0]*100\n",
    "    Test_accuracy = score[1]*100\n",
    "    \n",
    "    cohen = cohen_kappa_score(np.argmax(y_test, axis=1), y_pred)\n",
    "    \n",
    "    return classification, confusion, Test_Loss, Test_accuracy, cohen, oa\n",
    "\n",
    "\n",
    "classification, confusion, Test_loss, Test_accuracy, cohen, oa = reports(X_test_RNN, X_test_RES, y_test)\n",
    "classification = str(classification)\n",
    "#confusion = str(confusion)\n",
    "file_name = 'Spatial_Spectral_Attention---' + \"WindowSize\" + str(windowSize) + '---test_size(%)' + str(testRatio) +'.txt'\n",
    "with open(file_name, 'w') as x_file:\n",
    "    #x_file.write('{} Test loss (%)'.format(Test_loss))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Test accuracy (%)'.format(Test_accuracy))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Kappa score (%)'.format(cohen*100))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{} Overall Accuracy (%)'.format(oa*100))\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('\\n')\n",
    "    x_file.write('{}'.format(classification))\n",
    "    x_file.write('\\n')\n",
    "    #x_file.write('{}'.format(confusion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
